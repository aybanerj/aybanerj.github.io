---
layout: publication
title: "P2SLAM: Bearing based WiFi SLAM for Indoor Robots"
short_title: "P2SLAM"
tags: Wireless-Sensing
cover: /assets/images/pubpic/p2slam.png
authors: "Aditya Arun, Roshan Ayyalasomayajula, William Hunter, Dinesh Bharadia"
author_list:
    - name: Aditya Arun
      url: https://wcsng.ucsd.edu/aarun/
      email: aarun@ucsd.edu
    - name: Roshan Ayyalasomayajula
      url: https://engineering.buffalo.edu/computer-science-engineering/people/faculty-directory/full-time.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/ladder/ayyalasomayajula-roshan.detail.html
      email: roshana@buffalo.edu
    - name: William Hunter
      email: wshunter@ucsd.edu
    - name: Dinesh Bharadia
      url: https://dineshb-ucsd.github.io/
      email: dineshb@ucsd.edu
conference: "IEEE Robotics and Automation 2022"
conference_site: https://ieeexplore.ieee.org/document/9691786
paper: /files/p2slam.pdf
video: https://www.youtube.com/embed/JjalvBHqC94
video_str: "Teaser"
miscs: 
  - content_type: "Dataset"
    content_url: https://forms.gle/yMgrBwLHvz97ifep8  
press:
  date: "November 20, 2020"
  url: https://today.ucsd.edu/story/using-everyday-wifi-to-help-robots-see-and-navigate-better-indoors
  headline: Using Everyday WiFi To Help Robots See and Navigate Better Indoors

osd: "Exteroceptive sensors, like cameras and Lidars, when used for Robot SLAM, are deficient in highly structured environments and dynamic lighting conditions. This letter will present WiFi as a robust and straightforward sensing modality capable of circumventing these issues. Specifically, we make three contributions. First, we will understand the necessary features to be extracted from WiFi signals. Second, we characterize the quality of these measurements. Third, we integrate these features with odometry into a state-of-art GraphSLAM backend. We present our results in a 25×30 m and 50×40 environment and robustly test the system by driving the robot a cumulative distance of over 1225 m in these two environments. We show an improvement of at least 6× compared odometry-only estimation and perform on par with one of the state-of-the-art Visual-based SLAM."

description: 
  - text: "A recent spur of interest in indoor robotics has increased the importance of robust simultaneous localization and mapping algorithms in indoor scenarios. This robustness is typically provided by the use of multiple sensors which can correct each others’ deficiencies. In this vein, exteroceptive sensors, like cameras and LiDAR’s, employed for fusion are capable of correcting the drifts accumulated by wheel odometry or inertial measurement units (IMU’s). However, these exteroceptive sensors are deficient in highly structured environments and dynamic lighting conditions. This letter will present WiFi as a robust and straightforward sensing modality capable of circumventing these issues. Specifically, we make three contributions. First, we will understand the necessary features to be extracted from WiFi signals. Second, we characterize the quality of these mea surements. Third, we integrate these features with odometry into a state-of-art GraphSLAM backend. We present our results in a 25×30 m and 50×40 environment and robustly test the system by driving the robot a cumulative distance of over 1225 m in these two environments. We show an improvement of at least 6× compared odometry-only estimation and perform on par with one of the state-of-the-art Visual-based SLAM."
  - title: High Level Idea
    text: 
  - title: Environment Description 
    text: 
  - title: In-depth Results 
    text: 
  - title: Swap-C Analysis
    text: 
  - title: Dataset Usage
    text: 
---
The CSI data is named as **channels.mat** and the rosbag is named as **data.bag** in the resepctive dataset folders. All the datasets can be downloaded [here (size: 19.6 GB)](https://ucsdcloud-my.sharepoint.com/:u:/g/personal/aarun_ucsd_edu/Ed8UkFdFa71AqorqwJM-ESkBPJaC8z18bSKXob4A29K14A). To access the dataset, you will need to fill out a brief <a href="https://forms.gle/UgUQ5XkhVLqE9FEj6" target="_blank">Google Form</a> to agree to the terms of usage. Upon form completion an email will be sent out with the download instructions. 

The MATLAB files (channels.mat) are stored using **HDF5** file structure and contain the following variables:
- **channels_cli**: *[ n_datapoints x n_frequency x n_ap x n_rx_ant X n_tx_ant]* 5D complex channel matrix recieved at the WiFi access points deployed in the environment.
- **channels_ap**: *[ n_datapoints x n_frequency x num_aps x n_rx_ant X n_tx_ant]* 5D complex channel matrix recieved at the WiFi access point present on the robot.
- **cli_rssi_synced**: *[ n_datapoints x n_rx_ant x n_ap ]* Recieved signal strength matrix at the environments' WiFi access point.
- **ap_rssi_synced**: *[ n_datapoints x n_rx_ant x n_ap ]* Recieved signal strength matrix at the robot's WiFi access point.
- **cli_hw_noise_synced**: *[ n_datapoints x n_ap ]* Hardware noise floor measured at the Environment's AP's when signal is received from the robot.
- **ap_hw_noise_synced**: *[ n_datapoints x n_ap ]* Hardware noise floor measured at robot' WiFi AP with signal received from each of the environment's AP's.
- **ap**: *[1 x n_ap]* cell matrix. Each element corresponding to *[ n_rx_ant x 2]* antenna positions in th global coordinates.  
- **labels**: *[ n_datapoints x 3 ]* 2D best-estimate ground truth XY labels + heading of the robot -- computed using Cartographer using onboard Lidar and internal odometry.
- **labels_noise**: *[ n_datapoints x 3 ]* 2D XY labels + heading of the robot computed using only robot's wheel encoder (highly erroneous measurements)
- **labels_imu**: *[ n_datapoints x 3 ]* 2D  XY labels + heading of the robot computed using the robot's wheel encoder and internal gyroscope (less erroneous than only using wheel encoders)
- **labels_vel**: *[ n_datapoints x 1 ]* Velocity of the robot in m/s.

The rosbags (data.bag) contain the following topics:

1. **Camera Information from Intel Realsense D415. DS 1 uses the Orbbec Astra camera and its topics are in parantheses**:
- /camera/color/camera_info (/camera/rgb/camera_info)
- /camera/color/image_raw/compressed (/camera/rgb/image_rect_color/compressed)
- /camera/depth_registered/image_raw/compressedDepth (/camera/depth_registered/image_raw/compressedDepth)

2. **Odometry Information from Turtlebot base**:
- /mobile_base/sensors/core
- /mobile_base/sensors/imu_data
- /mobile_base/sensors/imu_data_raw
- /odom
- /tf

3. **Hokuoyo Lidar scan information**:
- /scan
