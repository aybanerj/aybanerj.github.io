---
layout: publication
title: "WAIS: Leveraging WiFi for Resource-Efficient SLAM"
short_title: "WAIS"
tags: Wireless-Sensing 
cover: /assets/images/pubpic/viwid.png
authors: "Aditya Arun, William Hunter, Roshan Ayyalasomayajula, Dinesh Bharadia"
author_list:
    - name: Aditya Arun
      url: https://wcsng.ucsd.edu/aarun/
      email: aarun@ucsd.edu
    - name: William Hunter
      email: wshunter@ucsd.edu
    - name: Roshan Ayyalasomayajula
      url: https://www.acsu.buffalo.edu/~roshana/
      email: roshana@buffalo.edu
    - name: Dinesh Bharadia
      email: dineshb@uscd.edu
      url: https://dineshb-ucsd.github.io/ 
conference: "Mobisys 2024"
conference_site: https://www.sigmobile.org/mobisys/2024/accepted-papers.html
paper: /files/wais.pdf
video: https://www.youtube.com/embed/FIAdBC_UzPU
video_str: Demo
miscs: 
    - content_type: Github
      content_url: https://github.com/ucsdwcsng/wais
    - content_type: Dataset
      content_url: https://forms.gle/KvgBy46mNNozneeXA
description:
    - title: Overview 
      text: Visual (LiDAR/camera-based) SLAM systems deploy compute and memory-intensive search algorithms to detect "Loop Closures" to make the trajectory estimate globally consistent. Instead, WAIS (WiFi Assisted Indoor SLAM) demonstrates using WiFi-based sensing can reduce this resource intensiveness drastically. By covering over 1500 m in realistic indoor environments and WiFi deployments, we showcase 4.3 x and 4 x reduction in compute and memory consumption compared to state-of-the-art Visual and Lidar SLAM systems. Incorporating WiFi into the sensor stack also improves the resiliency of the Visual-SLAM system. We find the 90th percentile translation errors improve by ~40% and orientation errors by ~60% compared with purely camera-based systems.
      image: /assets/images/pubpic/wais_overview.png
      image_width: 800

    - title: New Dataset Added to P2SLAM
      text: 
        In addition to the environments described in P2SLAM, we have collected a new dataset in an office envrionment, with stereo camera data, IMU, Lidar, Wheel odometry and WiFi measurements. The purple box and orange boxes indicate the robot and the APâ€™s placed in the environment respectively. The colored boxes in the map correspond to the view points of the 3 images of the environment shown, color-matched to the edges of the respective images.
      image: "/assets/images/pubpic/viwid-env.png"
      image_width: 800
---
